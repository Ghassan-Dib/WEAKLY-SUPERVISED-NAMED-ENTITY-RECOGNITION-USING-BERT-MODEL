{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0510 17:03:46.905205 139825465988864 file_utils.py:41] PyTorch version 1.3.0 available.\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from conlleval import eval_f1score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModel, BertPreTrainedModel, BertModel, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"data/train/train_dataset.txt\"\n",
    "PATH_VAL = \"data/val/val_dataset.txt\"\n",
    "PATH_AQMAR_TEST = \"data/test/aqmar_test_dataset.txt\"\n",
    "PATH_NEWS_TEST = \"data/test/news_test_dataset.txt\"\n",
    "PATH_TWEETS_TEST = \"data/test/tweets_test_dataset.txt\"\n",
    "PATH_SEMILABELED = \"data/semi_labeled/semi_labeled_dataset.txt\"\n",
    "PATH_STUDENT = \"data/student_dataset.txt\"\n",
    "\n",
    "FULL_FINETUNE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {\"O\":0, \"B-ORG\":1, \"I-ORG\":2, \"B-PER\":3, \"I-PER\":4, \"B-LOC\":5, \"I-LOC\":6}\n",
    "id_to_label = {value: key for key, value in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0510 17:03:48.630679 139825465988864 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/aubmindlab/bert-base-arabertv01/config.json from cache at /home/chadi/.cache/torch/transformers/edefbd57b711b1796edd80ad0058293ec6e302f92fba0fcdd7138805dc6164ab.f6fc50854095aaf1023a82f7d5210b2df75a0334997d2daf64453496246d7b2d\n",
      "I0510 17:03:48.631953 139825465988864 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 17029,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "I0510 17:03:48.632774 139825465988864 tokenization_utils.py:420] Model name 'aubmindlab/bert-base-arabertv01' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'aubmindlab/bert-base-arabertv01' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0510 17:04:12.646167 139825465988864 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/aubmindlab/bert-base-arabertv01/vocab.txt from cache at /home/chadi/.cache/torch/transformers/0ce662be054280545e93d3d0c9cb12a377f56a2dbdd8899a27b318676d220eab.8db01686d8f91fa1388a69dea94fd0e774a3a422cdb3f0f239148a2aefb0f405\n",
      "I0510 17:04:12.647930 139825465988864 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/aubmindlab/bert-base-arabertv01/added_tokens.json from cache at None\n",
      "I0510 17:04:12.649970 139825465988864 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/aubmindlab/bert-base-arabertv01/special_tokens_map.json from cache at None\n",
      "I0510 17:04:12.651849 139825465988864 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/aubmindlab/bert-base-arabertv01/tokenizer_config.json from cache at None\n"
     ]
    }
   ],
   "source": [
    "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv01\",do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_label(label):\n",
    "    if \"B-ORG\" in label:\n",
    "        return \"B-ORG\"\n",
    "    elif \"I-ORG\" in label:\n",
    "        return \"I-ORG\"\n",
    "    elif \"B-PER\" in label:\n",
    "        return \"B-PER\"\n",
    "    elif \"I-PER\" in label:\n",
    "        return \"I-PER\"\n",
    "    elif \"B-LOC\" in label:\n",
    "        return \"B-LOC\"\n",
    "    elif \"I-LOC\" in label:\n",
    "        return \"I-LOC\"\n",
    "    elif \"O\" in label:\n",
    "        return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(PATH_DATASET, tokenizer, max_length=512):\n",
    "    data = pd.read_csv(PATH_DATASET, encoding=\"utf-8\", delim_whitespace=True, usecols=[0,1], header=None, skip_blank_lines=False)\n",
    "    Instance = namedtuple(\"Instance\", [\"tokenized_text\", \"input_ids\", \"input_mask\", \"labels\", \"label_ids\"])\n",
    "    dataset = []\n",
    "    text = [\"[CLS]\"]\n",
    "    labels = [\"O\"]\n",
    "    for w, l in zip(data[0], data[1]):\n",
    "        if str(w) == \"nan\" and str(l) == \"nan\":\n",
    "            text.append(\"[SEP]\")\n",
    "            labels.append(\"O\")\n",
    "            \n",
    "            str_text = \" \".join(text)\n",
    "            tokenized_text = arabert_tokenizer.tokenize(str_text)\n",
    "            \n",
    "            cnt = 0 \n",
    "            new_labels = []\n",
    "            label_ids = []\n",
    "            for i in tokenized_text:\n",
    "                if \"##\" in i:\n",
    "                    tok_label = labels[cnt - 1]\n",
    "                    if \"B-\" in tok_label:\n",
    "                        tok_label = tok_label.replace(\"B-\", \"I-\")\n",
    "                        \n",
    "                    tok_label = clean_label(tok_label)\n",
    "                    new_labels.append(tok_label)\n",
    "                    label_ids.append(label_to_id[tok_label])\n",
    "                else:\n",
    "                    new_labels.append(labels[cnt])\n",
    "                    label_ids.append(label_to_id[clean_label(labels[cnt])])\n",
    "                    cnt += 1\n",
    "                                    \n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "                \n",
    "            input_mask = [1] * len(input_ids)\n",
    "            \n",
    "            while len(input_ids) < max_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                label_ids.append(label_to_id[\"O\"])\n",
    "            \n",
    "            dataset.append(Instance(tokenized_text, input_ids,\n",
    "                            input_mask, new_labels, label_ids))\n",
    "\n",
    "            text = [\"[CLS]\"]\n",
    "            labels = [\"O\"]\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        text.append(str(w))\n",
    "        labels.append(str(l))\n",
    "        \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_student_data(PATH_DATASET, tokenizer, max_length=512):\n",
    "    data = pd.read_csv(PATH_DATASET, encoding=\"utf-8\", delim_whitespace=True, header=None, skip_blank_lines=False, error_bad_lines=False)\n",
    "    Instance = namedtuple(\"Instance\", [\"tokenized_text\", \"input_ids\", \"input_mask\", \"labels\", \"label_ids\"])\n",
    "    dataset = []\n",
    "    text = [\"[CLS]\"]\n",
    "    labels = [\"O\"]\n",
    "    for w, l in zip(data[0], data[1]):\n",
    "        if str(w) == \"nan\" and str(l) == \"nan\":\n",
    "            text.append(\"[SEP]\")\n",
    "            labels.append(\"O\")\n",
    "            \n",
    "            \n",
    "            label_ids = []\n",
    "            cnt = 0\n",
    "\n",
    "            for i in text:\n",
    "                tok_label = labels[cnt]\n",
    "                label_ids.append(label_to_id[tok_label])\n",
    "                cnt+=1\n",
    "                                    \n",
    "            input_ids = tokenizer.convert_tokens_to_ids(text)\n",
    "                \n",
    "            input_mask = [1] * len(input_ids)\n",
    "            \n",
    "            while len(input_ids) < max_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                label_ids.append(label_to_id[\"O\"])\n",
    "            \n",
    "            dataset.append(Instance(text, input_ids,\n",
    "                            input_mask, labels, label_ids))\n",
    "\n",
    "            text = [\"[CLS]\"]\n",
    "            labels = [\"O\"]\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        text.append(str(w))\n",
    "        labels.append(str(l))\n",
    "        \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_semilabeled(PATH_DATASET, tokenizer, max_length=512):\n",
    "    data = pd.read_csv(PATH_DATASET, encoding=\"utf-8\", delim_whitespace=True, header=None, skip_blank_lines=False, error_bad_lines=False)\n",
    "    Instance = namedtuple(\"Instance\", [\"tokenized_text\", \"input_ids\", \"input_mask\", \"labels\", \"label_ids\", \"proba_classes\"])\n",
    "    dataset = []\n",
    "    text = [\"[CLS]\"]\n",
    "    labels = [\"O\"]\n",
    "    proba_classes = [[0.0, 0.0, 0.0]]\n",
    "    cnt_error = 0\n",
    "    for ins in data.values:\n",
    "        if str(ins[0]) == \"nan\" and str(ins[1]) == \"nan\":\n",
    "            text.append(\"[SEP]\")\n",
    "            labels.append(\"O\")\n",
    "            proba_classes.append([0.0, 0.0, 0.0])\n",
    "            \n",
    "            str_text = \" \".join(text)\n",
    "            tokenized_text = arabert_tokenizer.tokenize(str_text)\n",
    "            \n",
    "            cnt = 0 \n",
    "            new_labels = []\n",
    "            new_proba_classes = []\n",
    "            label_ids = []\n",
    "            \n",
    "            if len(tokenized_text) < 512:\n",
    "                try:\n",
    "                    for i in tokenized_text:\n",
    "                        if \"##\" in i:\n",
    "                            tok_label = labels[cnt - 1]\n",
    "                            if \"B-\" in tok_label:\n",
    "                                tok_label = tok_label.replace(\"B-\", \"I-\")\n",
    "\n",
    "                            tok_label = clean_label(tok_label)\n",
    "                            new_labels.append(tok_label)\n",
    "                            new_proba_classes.append(proba_classes[cnt - 1])\n",
    "                            label_ids.append(label_to_id[tok_label])\n",
    "                        else:\n",
    "                            new_labels.append(labels[cnt])\n",
    "                            new_proba_classes.append(proba_classes[cnt])\n",
    "                            label_ids.append(label_to_id[clean_label(labels[cnt])])\n",
    "                            cnt += 1\n",
    "\n",
    "                    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    while len(input_ids) < max_length:\n",
    "                        input_ids.append(0)\n",
    "                        input_mask.append(0)\n",
    "                        label_ids.append(label_to_id[\"O\"])\n",
    "\n",
    "                    dataset.append(Instance(tokenized_text, input_ids,\n",
    "                                    input_mask, new_labels, label_ids, new_proba_classes))\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"An error occured\")\n",
    "                    cnt_error += 1\n",
    "                    \n",
    "            text = [\"[CLS]\"]\n",
    "            labels = [\"O\"]\n",
    "            proba_classes = [[0.0, 0.0, 0.0]]\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        text.append(str(ins[0]))\n",
    "        labels.append(str(ins[1]))\n",
    "        \n",
    "        temp_proba_classes = [str(p) for p in ins[2:]]\n",
    "        temp_proba_classes = \" \".join(temp_proba_classes).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        temp_proba_classes = [float(p) for p in temp_proba_classes.split(\" \")[1:]]\n",
    "        proba_classes.append(temp_proba_classes)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_semilabeled_without_proba(PATH_DATASET, tokenizer, max_length=512):\n",
    "    data = pd.read_csv(PATH_DATASET, encoding=\"utf-8\", delim_whitespace=True, usecols=[0,1], header=None, skip_blank_lines=False, error_bad_lines=False)\n",
    "    Instance = namedtuple(\"Instance\", [\"tokenized_text\", \"input_ids\", \"input_mask\", \"labels\", \"label_ids\"])\n",
    "    dataset = []\n",
    "    text = [\"[CLS]\"]\n",
    "    labels = [\"O\"]\n",
    "    cnt_error = 0\n",
    "    for ins in data.values:\n",
    "        if str(ins[0]) == \"nan\" and str(ins[1]) == \"nan\":\n",
    "            text.append(\"[SEP]\")\n",
    "            labels.append(\"O\")\n",
    "            str_text = \" \".join(text)\n",
    "            tokenized_text = arabert_tokenizer.tokenize(str_text)\n",
    "\n",
    "            cnt = 0 \n",
    "            new_labels = []\n",
    "            label_ids = []\n",
    "            \n",
    "            if len(tokenized_text) < 512:\n",
    "                try:\n",
    "                    for i in tokenized_text:\n",
    "                        if \"##\" in i:\n",
    "                            tok_label = labels[cnt - 1]\n",
    "                            if \"B-\" in tok_label:\n",
    "\n",
    "                                tok_label = tok_label.replace(\"B-\", \"I-\")\n",
    "\n",
    "                            tok_label = clean_label(tok_label)\n",
    "                            new_labels.append(tok_label)\n",
    "                            label_ids.append(label_to_id[tok_label])\n",
    "                        else:\n",
    "                            new_labels.append(labels[cnt])\n",
    "                            label_ids.append(label_to_id[clean_label(labels[cnt])])\n",
    "                            cnt += 1\n",
    "\n",
    "                    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    while len(input_ids) < max_length:\n",
    "                        input_ids.append(0)\n",
    "                        input_mask.append(0)\n",
    "                        label_ids.append(label_to_id[\"O\"])\n",
    "\n",
    "                    dataset.append(Instance(tokenized_text, input_ids,\n",
    "                                        input_mask, new_labels, label_ids))\n",
    "                except Exception as e:\n",
    "                    print(\"An error occured\")\n",
    "                    cnt_error += 1\n",
    "\n",
    "            text = [\"[CLS]\"]\n",
    "            labels = [\"O\"]\n",
    "            continue\n",
    "                \n",
    "        text.append(str(ins[0]))\n",
    "        labels.append(str(ins[1]))\n",
    "        \n",
    "    print(\"Errors:{}\".format(cnt_error))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_tensors(dataset):\n",
    "    tensors_input_ids = []\n",
    "    tensors_input_mask = []\n",
    "    tensors_label_ids = []\n",
    "    for i in dataset:\n",
    "        tensors_input_ids.append(i.input_ids)\n",
    "        tensors_input_mask.append(i.input_mask)\n",
    "        tensors_label_ids.append(i.label_ids)\n",
    "        \n",
    "    return torch.tensor(tensors_input_ids), torch.tensor(tensors_input_mask), torch.tensor(tensors_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_tensors_semilabeled(dataset):\n",
    "    tensors_input_ids = []\n",
    "    tensors_input_mask = []\n",
    "    for i in dataset:\n",
    "        tensors_input_ids.append(i.input_ids)\n",
    "        tensors_input_mask.append(i.input_mask)\n",
    "        \n",
    "    return torch.tensor(tensors_input_ids), torch.tensor(tensors_input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedBertForTokenClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=7):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs =  logits # (logits,) + outputs[2:] add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs =  loss # (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, filename, dataset, predict_dataloader, device=\"cpu\"):\n",
    "    global id_to_label\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fw =  open(\"{}\".format(filename), \"w\", encoding=\"utf-8\")\n",
    "        cnt = 0\n",
    "        for batch in tqdm(predict_dataloader):\n",
    "            input_ids, input_mask = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            output = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "            length = len(dataset[cnt].tokenized_text)\n",
    "            \n",
    "            for w in range(length):\n",
    "                word = dataset[cnt].tokenized_text[w]\n",
    "                if word != \"[CLS]\" and word != \"[SEP]\":\n",
    "                    if dataset[cnt].labels[w] != \"O\":\n",
    "                        pred_label = dataset[cnt].labels[w]\n",
    "                    else:\n",
    "                        pred_label = id_to_label[torch.argmax(output.squeeze(0)[w]).item()]\n",
    "                    \n",
    "                    fw.write(\"{} {} \\n\".format(word, pred_label))\n",
    "            fw.write(\"\\n\")\n",
    "            cnt += 1\n",
    "        fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, filename, dataset, dataloader):\n",
    "    global id_to_label\n",
    "    model.eval()\n",
    "    f1_score = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fw =  open(\"{}\".format(filename), \"w\")\n",
    "        cnt = 0\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids, input_mask, _ = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            output = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "            length = len(dataset[cnt].tokenized_text)\n",
    "            for w in range(length):\n",
    "                word = dataset[cnt].tokenized_text[w]\n",
    "                true_label = clean_label(dataset[cnt].labels[w])\n",
    "                pred_label = id_to_label[torch.argmax(output.squeeze(0)[w]).item()]\n",
    "                fw.write(\"{} {} {}\\n\".format(word, true_label, pred_label))\n",
    "            fw.write(\"\\n\")\n",
    "            cnt += 1\n",
    "        fw.close()\n",
    "\n",
    "        _, f1_score_arr = eval_f1score(\"{}\".format(filename))\n",
    "        \n",
    "    return f1_score_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, val_dataloader, dataset_val, accumulation_steps=32, epochs=1, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    best_f1_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        cnt_step = 0\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            \n",
    "            input_ids, input_mask, label_ids = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "            \n",
    "            loss = model(input_ids=input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "            training_loss += loss.data.item()\n",
    "            \n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (cnt_step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            cnt_step += 1\n",
    "\n",
    "        training_loss /= cnt_step\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader):\n",
    "                input_ids, input_mask, label_ids = batch\n",
    "                input_ids = input_ids.to(device)\n",
    "                input_mask = input_mask.to(device)\n",
    "                label_ids = label_ids.to(device)\n",
    "\n",
    "                loss = model(input_ids=input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "                val_loss += loss.data.item()\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "\n",
    "            print(\"epoch {}: training loss {}, val loss {}\".format(epoch, training_loss, val_loss))\n",
    "            \n",
    "        f1_score_arr = evaluate(model, \"val.txt\", dataset_val, val_dataloader)\n",
    "        \n",
    "        if f1_score_arr[3] > best_f1_score:\n",
    "                best_f1_score = f1_score_arr[3]\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print(\"We have a better model with an F1 Score: {}\".format(best_f1_score))\n",
    "            \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss(model, optimizer, train_dataloader, val_dataloader, dataset_val, accumulation_steps=32, epochs=1, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    best_loss = 10000\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        cnt_step = 0\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            \n",
    "            input_ids, input_mask, label_ids = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "            \n",
    "            loss = model(input_ids=input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "            training_loss += loss.data.item()\n",
    "            \n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (cnt_step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            cnt_step += 1\n",
    "\n",
    "        training_loss /= cnt_step\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader):\n",
    "                input_ids, input_mask, label_ids = batch\n",
    "                input_ids = input_ids.to(device)\n",
    "                input_mask = input_mask.to(device)\n",
    "                label_ids = label_ids.to(device)\n",
    "\n",
    "                loss = model(input_ids=input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "                val_loss += loss.data.item()\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "\n",
    "            print(\"epoch {}: training loss {}, val loss {}\".format(epoch, training_loss, val_loss))\n",
    "            \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            print(\"We have a better model\")\n",
    "            \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0510 17:04:14.496253 139825465988864 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/aubmindlab/bert-base-arabertv01/config.json from cache at /home/chadi/.cache/torch/transformers/edefbd57b711b1796edd80ad0058293ec6e302f92fba0fcdd7138805dc6164ab.f6fc50854095aaf1023a82f7d5210b2df75a0334997d2daf64453496246d7b2d\n",
      "I0510 17:04:14.498865 139825465988864 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 17029,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "I0510 17:04:15.514603 139825465988864 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/aubmindlab/bert-base-arabertv01/pytorch_model.bin from cache at /home/chadi/.cache/torch/transformers/7ddf9458c5114984a4f63a0c7e695796eb5700dc46f37b64a690853fafa11957.1ebd3840a1c2d4f803e2ba3b9cb34174da7b261fda86d6e4e80466ac9c64a098\n",
      "I0510 17:04:17.842582 139825465988864 modeling_utils.py:601] Weights of ModifiedBertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0510 17:04:17.843200 139825465988864 modeling_utils.py:607] Weights from pretrained model not used in ModifiedBertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "teacher_arabert_model = ModifiedBertForTokenClassification.from_pretrained(\"aubmindlab/bert-base-arabertv01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = preprocess_data(PATH_TRAIN, arabert_tokenizer)\n",
    "dataset_val = preprocess_data(PATH_VAL, arabert_tokenizer)\n",
    "dataset_aqmar_test = preprocess_data(PATH_AQMAR_TEST, arabert_tokenizer)\n",
    "dataset_news_test = preprocess_data(PATH_NEWS_TEST, arabert_tokenizer)\n",
    "dataset_tweets_test = preprocess_data(PATH_TWEETS_TEST, arabert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors_input_ids, train_tensors_input_mask, train_tensors_label_ids = transform_to_tensors(dataset_train)\n",
    "val_tensors_input_ids, val_tensors_input_mask, val_tensors_label_ids = transform_to_tensors(dataset_val)\n",
    "test_aqmar_tensors_input_ids, test_aqmar_tensors_input_mask, test_aqmar_tensors_label_ids = transform_to_tensors(dataset_aqmar_test)\n",
    "test_news_tensors_input_ids, test_news_tensors_input_mask, test_news_tensors_label_ids = transform_to_tensors(dataset_news_test)\n",
    "test_tweets_tensors_input_ids, test_tweets_tensors_input_mask, test_tweets_tensors_label_ids = transform_to_tensors(dataset_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_dataset = TensorDataset(train_tensors_input_ids, train_tensors_input_mask, train_tensors_label_ids)\n",
    "val_tensor_dataset = TensorDataset(val_tensors_input_ids, val_tensors_input_mask, val_tensors_label_ids)\n",
    "test_aqmar_tensor_dataset = TensorDataset(test_aqmar_tensors_input_ids, test_aqmar_tensors_input_mask, test_aqmar_tensors_label_ids)\n",
    "test_news_tensor_dataset = TensorDataset(test_news_tensors_input_ids, test_news_tensors_input_mask, test_news_tensors_label_ids)\n",
    "test_tweets_tensor_dataset = TensorDataset(test_tweets_tensors_input_ids, test_tweets_tensors_input_mask, test_tweets_tensors_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_tensor_dataset, batch_size=1)\n",
    "val_dataloader = DataLoader(val_tensor_dataset, batch_size=1)\n",
    "test_aqmar_dataloader = DataLoader(test_aqmar_tensor_dataset, batch_size=1)\n",
    "test_news_dataloader = DataLoader(test_news_tensor_dataset, batch_size=1)\n",
    "test_tweets_dataloader = DataLoader(test_tweets_tensor_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL FINETUNE\n"
     ]
    }
   ],
   "source": [
    "optimizer_grouped_parameters = None\n",
    "param_optimizer = list(teacher_arabert_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "if FULL_FINETUNE:\n",
    "    print('ALL FINETUNE')\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    print('NO ALL FINETUNE')\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': teacher_arabert_model.classifier.parameters(),\n",
    "         'weight_decay_rate': 0.01}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4147/4147 [04:59<00:00, 13.84it/s]\n",
      "100%|██████████| 2555/2555 [01:03<00:00, 40.07it/s]\n",
      "  0%|          | 2/4147 [00:00<03:56, 17.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: training loss 0.28163901283193493, val loss 0.11237545299599833\n",
      "We have a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4147/4147 [05:08<00:00, 13.43it/s]\n",
      "100%|██████████| 2555/2555 [01:04<00:00, 39.29it/s]\n",
      "  0%|          | 2/4147 [00:00<04:07, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: training loss 0.06832305082777003, val loss 0.12113542770089786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4147/4147 [05:07<00:00, 13.48it/s]\n",
      "100%|██████████| 2555/2555 [01:04<00:00, 39.48it/s]\n",
      "  0%|          | 2/4147 [00:00<04:12, 16.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: training loss 0.041857792907891905, val loss 0.1278511223064188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4147/4147 [05:08<00:00, 13.45it/s]\n",
      "100%|██████████| 2555/2555 [01:05<00:00, 39.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: training loss 0.02915042966799278, val loss 0.144541788735899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_teacher_model = train_loss(teacher_arabert_model, optimizer, train_dataloader, val_dataloader, dataset_val, epochs=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2555/2555 [01:07<00:00, 38.03it/s]\n",
      "  0%|          | 4/2456 [00:00<01:03, 38.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 94131 tokens with 3118 phrases; found: 3031 phrases; correct: 2317.\n",
      "accuracy:  97.35%; precision:  76.44%; recall:  74.31%; FB1:  75.36\n",
      "              LOC: precision:  83.33%; recall:  93.78%; FB1:  88.25  1104\n",
      "              ORG: precision:  54.79%; recall:  50.58%; FB1:  52.60  553\n",
      "              PER: precision:  79.62%; recall:  71.13%; FB1:  75.14  1374\n",
      "[88.24940047961631, 52.604166666666664, 75.13736263736264, 75.36184745487071]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2456/2456 [01:04<00:00, 37.86it/s]\n",
      "  1%|▏         | 4/292 [00:00<00:07, 37.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 88841 tokens with 2886 phrases; found: 2673 phrases; correct: 1710.\n",
      "accuracy:  95.65%; precision:  63.97%; recall:  59.25%; FB1:  61.52\n",
      "              LOC: precision:  70.76%; recall:  59.78%; FB1:  64.81  1067\n",
      "              ORG: precision:  26.58%; recall:  37.98%; FB1:  31.27  523\n",
      "              PER: precision:  75.35%; recall:  64.92%; FB1:  69.74  1083\n",
      "[64.80686695278969, 31.2710911136108, 69.74358974358975, 61.521856449001625]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [00:07<00:00, 38.23it/s]\n",
      "  1%|          | 5/982 [00:00<00:23, 41.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 17655 tokens with 1195 phrases; found: 1125 phrases; correct: 849.\n",
      "accuracy:  94.31%; precision:  75.47%; recall:  71.05%; FB1:  73.19\n",
      "              LOC: precision:  82.96%; recall:  70.88%; FB1:  76.44  311\n",
      "              ORG: precision:  56.62%; recall:  52.12%; FB1:  54.28  325\n",
      "              PER: precision:  83.23%; recall:  85.15%; FB1:  84.18  489\n",
      "[76.44444444444444, 54.27728613569322, 84.17786970010341, 73.18965517241381]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:25<00:00, 38.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 22133 tokens with 513 phrases; found: 418 phrases; correct: 269.\n",
      "accuracy:  96.45%; precision:  64.35%; recall:  52.44%; FB1:  57.79\n",
      "              LOC: precision:  79.03%; recall:  47.12%; FB1:  59.04  124\n",
      "              ORG: precision:  34.94%; recall:  29.00%; FB1:  31.69  83\n",
      "              PER: precision:  67.30%; recall:  69.27%; FB1:  68.27  211\n",
      "[59.036144578313255, 31.693989071038253, 68.26923076923077, 57.787325456498394]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[59.036144578313255, 31.693989071038253, 68.26923076923077, 57.787325456498394]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(trained_teacher_model, \"val.txt\", dataset_val, val_dataloader)\n",
    "evaluate(trained_teacher_model, \"test_aqmar.txt\", dataset_aqmar_test, test_aqmar_dataloader)\n",
    "evaluate(trained_teacher_model, \"test_news.txt\", dataset_news_test, test_news_dataloader)\n",
    "evaluate(trained_teacher_model, \"test_tweets.txt\", dataset_tweets_test, test_tweets_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModifiedBertForTokenClassification. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(trained_teacher_model, \"best_teacher_model.h5\")\n",
    "del trained_teacher_model\n",
    "del teacher_arabert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Semi-Labeled Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = torch.load(\"best_teacher_model.h5\",map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "An error occured\n",
      "Errors:2449\n"
     ]
    }
   ],
   "source": [
    "dataset_predict = preprocess_semilabeled_without_proba(PATH_SEMILABELED, arabert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tensors_input_ids, predict_tensors_input_mask = transform_to_tensors_semilabeled(dataset_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tensor_dataset = TensorDataset(predict_tensors_input_ids, predict_tensors_input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataloader = DataLoader(predict_tensor_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53711/53711 [23:45<00:00, 37.69it/s]\n"
     ]
    }
   ],
   "source": [
    "predict(teacher_model, PATH_STUDENT, dataset_predict, predict_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0510 17:56:17.824970 139825465988864 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/aubmindlab/bert-base-arabertv01/config.json from cache at /home/chadi/.cache/torch/transformers/edefbd57b711b1796edd80ad0058293ec6e302f92fba0fcdd7138805dc6164ab.f6fc50854095aaf1023a82f7d5210b2df75a0334997d2daf64453496246d7b2d\n",
      "I0510 17:56:17.825569 139825465988864 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 17029,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "I0510 17:56:17.828546 139825465988864 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/aubmindlab/bert-base-arabertv01/pytorch_model.bin from cache at /home/chadi/.cache/torch/transformers/7ddf9458c5114984a4f63a0c7e695796eb5700dc46f37b64a690853fafa11957.1ebd3840a1c2d4f803e2ba3b9cb34174da7b261fda86d6e4e80466ac9c64a098\n",
      "I0510 17:56:20.158277 139825465988864 modeling_utils.py:601] Weights of ModifiedBertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0510 17:56:20.158776 139825465988864 modeling_utils.py:607] Weights from pretrained model not used in ModifiedBertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "student_arabert_model = ModifiedBertForTokenClassification.from_pretrained(\"aubmindlab/bert-base-arabertv01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_dataset_train = preprocess_student_data(PATH_STUDENT, arabert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_train_tensors_input_ids, student_train_tensors_input_mask, student_train_tensors_label_ids = transform_to_tensors(student_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_train_tensor_dataset = TensorDataset(student_train_tensors_input_ids, student_train_tensors_input_mask, student_train_tensors_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_train_dataloader = DataLoader(student_train_tensor_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL FINETUNE\n"
     ]
    }
   ],
   "source": [
    "optimizer_grouped_parameters = None\n",
    "param_optimizer = list(student_arabert_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "if FULL_FINETUNE:\n",
    "    print('ALL FINETUNE')\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    print('NO ALL FINETUNE')\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': student_arabert_model.classifier.parameters(),\n",
    "         'weight_decay_rate': 0.01}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53711/53711 [1:06:06<00:00, 13.54it/s]\n",
      "100%|██████████| 2555/2555 [01:05<00:00, 38.96it/s]\n",
      "  0%|          | 2/53711 [00:00<52:40, 16.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: training loss 0.24410190692526343, val loss 0.1390509720883968\n",
      "We have a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53711/53711 [1:05:52<00:00, 13.59it/s]\n",
      "100%|██████████| 2555/2555 [01:05<00:00, 39.17it/s]\n",
      "  0%|          | 2/53711 [00:00<54:34, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: training loss 0.16812767760870434, val loss 0.1580906663756816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53711/53711 [1:05:47<00:00, 13.61it/s]\n",
      "100%|██████████| 2555/2555 [01:04<00:00, 39.36it/s]\n",
      "  0%|          | 2/53711 [00:00<55:27, 16.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: training loss 0.13343738509957978, val loss 0.16918142978833267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53711/53711 [1:05:49<00:00, 13.60it/s]\n",
      "100%|██████████| 2555/2555 [01:05<00:00, 39.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: training loss 0.1063113289395121, val loss 0.17889171409619556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "student_teacher_model = train_loss(student_arabert_model, optimizer, student_train_dataloader, val_dataloader, dataset_val, epochs=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2555/2555 [01:07<00:00, 37.91it/s]\n",
      "  0%|          | 4/2456 [00:00<01:07, 36.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 94131 tokens with 3118 phrases; found: 3589 phrases; correct: 2352.\n",
      "accuracy:  96.53%; precision:  65.53%; recall:  75.43%; FB1:  70.14\n",
      "              LOC: precision:  64.77%; recall:  91.85%; FB1:  75.97  1391\n",
      "              ORG: precision:  45.35%; recall:  60.27%; FB1:  51.76  796\n",
      "              PER: precision:  77.75%; recall:  70.87%; FB1:  74.15  1402\n",
      "[75.96964586846543, 51.75627240143369, 74.14965986394556, 70.13567914119577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2456/2456 [01:04<00:00, 38.83it/s]\n",
      "  1%|▏         | 4/292 [00:00<00:07, 36.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 88841 tokens with 2886 phrases; found: 3352 phrases; correct: 1909.\n",
      "accuracy:  95.33%; precision:  56.95%; recall:  66.15%; FB1:  61.21\n",
      "              LOC: precision:  62.54%; recall:  69.28%; FB1:  65.74  1399\n",
      "              ORG: precision:  23.25%; recall:  51.64%; FB1:  32.06  813\n",
      "              PER: precision:  74.12%; recall:  67.22%; FB1:  70.50  1140\n",
      "[65.74004507888806, 32.06106870229008, 70.50479766374634, 61.20551458800898]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [00:07<00:00, 37.07it/s]\n",
      "  0%|          | 4/982 [00:00<00:25, 37.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 17655 tokens with 1195 phrases; found: 1255 phrases; correct: 914.\n",
      "accuracy:  94.14%; precision:  72.83%; recall:  76.49%; FB1:  74.61\n",
      "              LOC: precision:  72.65%; recall:  74.45%; FB1:  73.54  373\n",
      "              ORG: precision:  56.25%; recall:  61.19%; FB1:  58.62  384\n",
      "              PER: precision:  85.74%; recall:  89.33%; FB1:  87.50  498\n",
      "[73.54138398914517, 58.61601085481682, 87.5, 74.61224489795919]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:26<00:00, 37.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 22133 tokens with 513 phrases; found: 571 phrases; correct: 307.\n",
      "accuracy:  95.83%; precision:  53.77%; recall:  59.84%; FB1:  56.64\n",
      "              LOC: precision:  60.66%; recall:  53.37%; FB1:  56.78  183\n",
      "              ORG: precision:  36.21%; recall:  42.00%; FB1:  38.89  116\n",
      "              PER: precision:  56.62%; recall:  75.12%; FB1:  64.57  272\n",
      "[56.77749360613811, 38.888888888888886, 64.57023060796645, 56.642066420664214]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[56.77749360613811, 38.888888888888886, 64.57023060796645, 56.642066420664214]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(student_teacher_model, \"student_val.txt\", dataset_val, val_dataloader)\n",
    "evaluate(student_teacher_model, \"student_test_aqmar.txt\", dataset_aqmar_test, test_aqmar_dataloader)\n",
    "evaluate(student_teacher_model, \"student_test_news.txt\", dataset_news_test, test_news_dataloader)\n",
    "evaluate(student_teacher_model, \"student_test_tweets.txt\", dataset_tweets_test, test_tweets_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL FINETUNE\n"
     ]
    }
   ],
   "source": [
    "optimizer_grouped_parameters = None\n",
    "param_optimizer = list(student_teacher_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "if FULL_FINETUNE:\n",
    "    print('ALL FINETUNE')\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    print('NO ALL FINETUNE')\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': student_teacher_model.classifier.parameters(),\n",
    "         'weight_decay_rate': 0.01}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4147/4147 [05:03<00:00, 13.65it/s]\n",
      "100%|██████████| 2555/2555 [01:04<00:00, 39.55it/s]\n",
      "  0%|          | 2/4147 [00:00<04:01, 17.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: training loss 0.0769748305420798, val loss 0.0996225939988074\n",
      "We have a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4147/4147 [05:05<00:00, 13.57it/s]\n",
      "100%|██████████| 2555/2555 [01:04<00:00, 39.49it/s]\n",
      "  0%|          | 2/4147 [00:00<04:14, 16.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: training loss 0.04176969075727229, val loss 0.11419789121150101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4147/4147 [05:05<00:00, 13.57it/s]\n",
      "100%|██████████| 2555/2555 [01:04<00:00, 39.38it/s]\n",
      "  0%|          | 2/4147 [00:00<04:12, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: training loss 0.026019873369021472, val loss 0.14112530970352088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4147/4147 [05:03<00:00, 13.66it/s]\n",
      "100%|██████████| 2555/2555 [01:04<00:00, 39.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: training loss 0.018926680339868194, val loss 0.1598931491144531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuned_student_teacher_model = train_loss(student_teacher_model, optimizer, train_dataloader, val_dataloader, dataset_val, epochs=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2555/2555 [01:07<00:00, 37.94it/s]\n",
      "  0%|          | 4/2456 [00:00<01:06, 37.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 94131 tokens with 3118 phrases; found: 3014 phrases; correct: 2405.\n",
      "accuracy:  97.58%; precision:  79.79%; recall:  77.13%; FB1:  78.44\n",
      "              LOC: precision:  87.16%; recall:  95.51%; FB1:  91.15  1075\n",
      "              ORG: precision:  62.19%; recall:  57.93%; FB1:  59.98  558\n",
      "              PER: precision:  81.17%; recall:  72.89%; FB1:  76.81  1381\n",
      "[91.14785992217898, 59.98271391529819, 76.80712572798905, 78.44096542726679]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2456/2456 [01:06<00:00, 36.79it/s]\n",
      "  1%|▏         | 4/292 [00:00<00:08, 34.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 88841 tokens with 2886 phrases; found: 2653 phrases; correct: 1776.\n",
      "accuracy:  95.96%; precision:  66.94%; recall:  61.54%; FB1:  64.13\n",
      "              LOC: precision:  75.27%; recall:  61.20%; FB1:  67.51  1027\n",
      "              ORG: precision:  26.69%; recall:  40.98%; FB1:  32.33  562\n",
      "              PER: precision:  80.17%; recall:  67.86%; FB1:  73.50  1064\n",
      "[67.51091703056768, 32.327586206896555, 73.50280051701851, 64.12709875428779]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [00:07<00:00, 38.26it/s]\n",
      "  0%|          | 4/982 [00:00<00:24, 39.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 17655 tokens with 1195 phrases; found: 1134 phrases; correct: 896.\n",
      "accuracy:  94.81%; precision:  79.01%; recall:  74.98%; FB1:  76.94\n",
      "              LOC: precision:  84.49%; recall:  73.35%; FB1:  78.53  316\n",
      "              ORG: precision:  62.50%; recall:  56.66%; FB1:  59.44  320\n",
      "              PER: precision:  86.14%; recall:  89.75%; FB1:  87.91  498\n",
      "[78.52941176470587, 59.43536404160476, 87.90983606557377, 76.94289394589954]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:25<00:00, 37.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 22133 tokens with 513 phrases; found: 510 phrases; correct: 299.\n",
      "accuracy:  95.88%; precision:  58.63%; recall:  58.28%; FB1:  58.46\n",
      "              LOC: precision:  82.68%; recall:  50.48%; FB1:  62.69  127\n",
      "              ORG: precision:  38.18%; recall:  42.00%; FB1:  40.00  110\n",
      "              PER: precision:  55.68%; recall:  74.15%; FB1:  63.60  273\n",
      "[62.686567164179095, 40.00000000000001, 63.59832635983263, 58.45552297165201]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[62.686567164179095, 40.00000000000001, 63.59832635983263, 58.45552297165201]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(finetuned_student_teacher_model, \"finetuned_student_val.txt\", dataset_val, val_dataloader)\n",
    "evaluate(finetuned_student_teacher_model, \"finetuned_student_test_aqmar.txt\", dataset_aqmar_test, test_aqmar_dataloader)\n",
    "evaluate(finetuned_student_teacher_model, \"finetuned_student_test_news.txt\", dataset_news_test, test_news_dataloader)\n",
    "evaluate(finetuned_student_teacher_model, \"finetuned_student_test_tweets.txt\", dataset_tweets_test, test_tweets_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModifiedBertForTokenClassification. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/chadi/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(finetuned_student_teacher_model, \"best_finetuned_student_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
